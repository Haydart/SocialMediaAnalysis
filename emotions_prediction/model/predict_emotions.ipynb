{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import twint\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/r.makowiecki/PycharmProjects/SocialMediaAnalysis\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "dir = './emotions_prediction'\n",
    "os.chdir(\"SocialMediaAnalysis\")\n",
    "print(os.getcwd())\n",
    "\n",
    "with open(\"{}/data/dataset.raw.pickle\".format(dir), \"rb\") as dataset_file:\n",
    "    dataset = pickle.load(dataset_file, encoding='latin1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n[{'label': array([1., 0., 0., 0., 0., 0., 0.])}, {'label': array([0., 1., 0., 0., 0., 0., 0.])}, {'label': array([0., 0., 1., 0., 0., 0., 0.])}, {'label': array([0., 0., 0., 1., 0., 0., 0.])}, {'label': array([0., 0., 0., 0., 1., 0., 0.])}, {'label': array([0., 0., 0., 0., 0., 1., 0.])}, {'label': array([0., 0., 0., 0., 0., 0., 1.])}, {'label': array([1., 0., 0., 0., 0., 0., 0.])}, {'label': array([0., 1., 0., 0., 0., 0., 0.])}, {'label': array([0., 0., 1., 0., 0., 0., 0.])}]\n['During the period of falling in love, each time that we met and especially when we had not met for a long time.', 'When I was involved in a traffic accident.', 'When I was driving home after  several days of hard work, there was a motorist ahead of me who was driving at 50 km/hour and refused, despite his low speeed to let me overtake.', 'When I lost the person who meant the most to me.', \"The time I knocked a deer down - the sight of the animal's injuries and helplessness.  The realization that the animal was so badly hurt that it had to be put down, and when the animal screamed at the moment of death.\", 'When I did not speak the truth.', 'When I caused problems for somebody because he could not keep the appointed time and this led to various consequences.', 'When I got a letter offering me the Summer job that I had applied for.', 'When I was going home alone one night in Paris and a man came up behind me and asked me if I was not afraid to be out alone so late at night.', 'When I was talking to HIM at a party for the first time in a long while and a friend came and interrupted us and HE left.']\n"
     ]
    }
   ],
   "source": [
    "labels = ['joy', 'fear', 'anger', 'sadness', 'disgust', 'shame', 'guilt']\n",
    "\n",
    "# print dataset type and sample\n",
    "print(type(dataset))\n",
    "print(dataset['info'][:10])\n",
    "print(dataset['texts'][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1., 0., 0., 0., 0., 0., 0.]), array([0., 1., 0., 0., 0., 0., 0.]), array([0., 0., 1., 0., 0., 0., 0.]), array([0., 0., 0., 1., 0., 0., 0.]), array([0., 0., 0., 0., 1., 0., 0.]), array([0., 0., 0., 0., 0., 1., 0.]), array([0., 0., 0., 0., 0., 0., 1.]), array([1., 0., 0., 0., 0., 0., 0.]), array([0., 1., 0., 0., 0., 0., 0.]), array([0., 0., 1., 0., 0., 0., 0.])]\n"
     ]
    }
   ],
   "source": [
    "X = dataset['texts']\n",
    "Y = [item['label'] for item in dataset['info']]\n",
    "    \n",
    "#print labels sample\n",
    "print(Y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_english_stopwords():\n",
    "    file_path = './emotions_prediction/data/nltk_english_stopwords'\n",
    "    with open(file_path, 'r') as stop_words_file:\n",
    "        return stop_words_file.read().splitlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']\n"
     ]
    }
   ],
   "source": [
    "stop_words = get_english_stopwords()\n",
    "print(stop_words)\n",
    "\n",
    "def vectorize_sentence(model_w2v, sentence, stop_words):\n",
    "    words = gensim.utils.simple_preprocess(sentence)\n",
    "\n",
    "    sentence_vector = []\n",
    "    for word in words:\n",
    "        if word not in stop_words and word in model_w2v.vocab:\n",
    "            sentence_vector.append(model_w2v[word])\n",
    "    sentence_vector = np.stack(sentence_vector, axis=0)\n",
    "\n",
    "    return np.mean(sentence_vector, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
